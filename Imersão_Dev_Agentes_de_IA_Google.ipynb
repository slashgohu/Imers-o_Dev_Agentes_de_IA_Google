{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 01**"
      ],
      "metadata": {
        "id": "t8fuksLJcIFK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "adgjoQICV1f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4668281-8b64-46fc-88b1-3a2e619918f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: langchain 1.0.8\n",
            "Uninstalling langchain-1.0.8:\n",
            "  Successfully uninstalled langchain-1.0.8\n",
            "Found existing installation: langchain-google-genai 0.0.1\n",
            "Uninstalling langchain-google-genai-0.0.1:\n",
            "  Successfully uninstalled langchain-google-genai-0.0.1\n",
            "Found existing installation: langchain-community 0.4.1\n",
            "Uninstalling langchain-community-0.4.1:\n",
            "  Successfully uninstalled langchain-community-0.4.1\n",
            "Found existing installation: langchain-text-splitters 1.0.0\n",
            "Uninstalling langchain-text-splitters-1.0.0:\n",
            "  Successfully uninstalled langchain-text-splitters-1.0.0\n",
            "Found existing installation: google-generativeai 0.3.2\n",
            "Uninstalling google-generativeai-0.3.2:\n",
            "  Successfully uninstalled google-generativeai-0.3.2\n"
          ]
        }
      ],
      "source": [
        "pip uninstall -y langchain langchain-google-genai langchain-community langchain-text-splitters google-generativeai && pip install -q --upgrade langchain langchain-google-genai langchain-community langchain-text-splitters google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import key do google\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "google_api_key = userdata.get('gemini_api_key')"
      ],
      "metadata": {
        "id": "XLREu-GBX77l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "f4c3fef9-57ac-4862-9d37-db82860e1ac4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_core.pydantic_v1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2475263952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import key do google\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_google_genai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatGoogleGenerativeAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgoogle_api_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemini_api_key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_google_genai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatGoogleGenerativeAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"ChatGoogleGenerativeAI\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChatGenerationChunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChatResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_validator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_from_dict_or_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m from tenacity import (\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.pydantic_v1'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configs do modelo\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model= 'gemini-2.5-flash',\n",
        "    temperature=0,\n",
        "    api_key=google_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "ejGQRM0wY0i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# teste do gemini, perguntando quem ele eh\n",
        "# .content n gera log de parametros\n",
        "resp_test = llm.invoke('Quem e voce? seja criativo')\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "id": "vJJfuSI9crjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triagem_prompt = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "BivE4kz6c9lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "class TriagemOut(BaseModel):\n",
        "  decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
        "  urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
        "  campos_faltantes: List[str] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "BJALA86ihDjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model= 'gemini-2.5-flash',\n",
        "    temperature=0,\n",
        "    api_key=google_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "CNk4XjRaijA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "def triagem(mensagem: str) -> Dict:\n",
        "  saida: TriagemOut = triagem_chain.invoke([\n",
        "      SystemMessage(content=triagem_prompt),\n",
        "      HumanMessage(content=mensagem)\n",
        "  ])\n",
        "\n",
        "  return saida.model_dump()"
      ],
      "metadata": {
        "id": "U4cEaExNi_Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto, como faco?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da alura?\",\n",
        "          \"Quantas capivaras tem o rio pinheiros?\"]"
      ],
      "metadata": {
        "id": "bkSpxdx7lYVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "  print(f'pergunta: {msg_teste}\\n -> resposta: {triagem(msg_teste)}')"
      ],
      "metadata": {
        "id": "xC__BWTSl7dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 02**"
      ],
      "metadata": {
        "id": "gwG17ZHcZOR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade faiss-cpu pymupdf"
      ],
      "metadata": {
        "id": "m72C8BlVZRV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f986de98"
      },
      "source": [
        "pip uninstall -y langchain langchain-google-genai langchain-community langchain-text-splitters google-generativeai && pip install -q --upgrade langchain langchain-google-genai langchain-community langchain-text-splitters google-generativeai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f50c7b74"
      },
      "source": [
        "!pip install -q --upgrade faiss-cpu pymupdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abc9cd92"
      },
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. \"\n",
        "     \"Responda SOMENTE com base no contexto fornecido. \"\n",
        "     \"Se não houver base suficiente, responda apenas 'Não sei'.\"),\n",
        "\n",
        "    (\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\")\n",
        "])\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "720c8c4d"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your folder in Google Drive\n",
        "drive_folder_path = '/content/drive/MyDrive/Estudos/Imersão Dev Agentes de IA Google/PDFs Aula 2'\n",
        "\n",
        "# Define the destination folder in your Colab environment\n",
        "colab_folder_path = '/content/pdfs'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "if not os.path.exists(colab_folder_path):\n",
        "    os.makedirs(colab_folder_path)\n",
        "\n",
        "# Copy PDF files from Google Drive to Colab environment\n",
        "for filename in os.listdir(drive_folder_path):\n",
        "    if filename.endswith('.pdf'):\n",
        "        shutil.copy(os.path.join(drive_folder_path, filename), colab_folder_path)\n",
        "\n",
        "print(f\"Copied PDF files from '{drive_folder_path}' to '{colab_folder_path}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs = []\n",
        "\n",
        "for n in Path('/content/pdfs').glob('*.pdf'):\n",
        "    try:\n",
        "        loader = PyMuPDFLoader(str(n))\n",
        "\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "        print(f'Arquivo carregado com sucesso: {n.name}')\n",
        "    except Exception as e:\n",
        "        print(f'Erro ao carregar o arquivo {n.name}: {e}')\n",
        "\n",
        "print(f'Total de docs carregados: {len(docs)}')"
      ],
      "metadata": {
        "id": "f8sFm7QNbCE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
        "\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "chunks"
      ],
      "metadata": {
        "id": "PM_sfqLhgGyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chunks:\n",
        "   print(chunks)\n",
        "   print('---------------------------------------/n')\n",
        "\n",
        "# The 'chunks' variable already contains the list of document chunks.\n",
        "# You can uncomment the line below to display the chunks if needed.\n",
        "# display(chunks)"
      ],
      "metadata": {
        "id": "p4uAz6tjhozx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model='models/gemini-embedding-001',\n",
        "    google_api_key=google_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "uXOEDMc3icyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type='similarity_score_threshold',\n",
        "                                     search_kwargs={'score_threshold':0.3, 'k': 4})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FPXYMsgmlKJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. \"\n",
        "     \"Responda SOMENTE com base no contexto fornecido. \"\n",
        "     \"Se não houver base suficiente, responda apenas 'Não sei'.\"),\n",
        "\n",
        "    (\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\")\n",
        "])\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag)"
      ],
      "metadata": {
        "id": "eqcRGs0GtfR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, pathlib\n",
        "\n",
        "def _clean_text(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "\n",
        "def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:\n",
        "    txt = _clean_text(texto)\n",
        "    termos = [t.lower() for t in re.findall(r\"\\w+\", query or \"\") if len(t) >= 4]\n",
        "    pos = -1\n",
        "    for t in termos:\n",
        "        pos = txt.lower().find(t)\n",
        "        if pos != -1: break\n",
        "    if pos == -1: pos = 0\n",
        "    ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2)\n",
        "    return txt[ini:fim]\n",
        "\n",
        "def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:\n",
        "    cites, seen = [], set()\n",
        "    for d in docs_rel:\n",
        "        src = pathlib.Path(d.metadata.get(\"source\",\"\")).name\n",
        "        page = int(d.metadata.get(\"page\", 0)) + 1\n",
        "        key = (src, page)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        cites.append({\"documento\": src, \"pagina\": page, \"trecho\": extrair_trecho(d.page_content, query)})\n",
        "    return cites[:3]"
      ],
      "metadata": {
        "id": "w43MyCWDt-eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perguntar_politica_RAG(pergunta: str) -> Dict:\n",
        "    docs_relacionados = retriever.invoke(pergunta)\n",
        "\n",
        "    if not docs_relacionados:\n",
        "        return {\"answer\": \"Não sei.\",\n",
        "                \"citacoes\": [],\n",
        "                \"contexto_encontrado\": False}\n",
        "\n",
        "    answer = document_chain.invoke({\"input\": pergunta,\n",
        "                                    \"context\": docs_relacionados})\n",
        "\n",
        "    txt = (answer or \"\").strip()\n",
        "\n",
        "    if txt.rstrip(\".!?\") == \"Não sei\":\n",
        "        return {\"answer\": \"Não sei.\",\n",
        "                \"citacoes\": [],\n",
        "                \"contexto_encontrado\": False}\n",
        "\n",
        "    return {\"answer\": txt,\n",
        "            \"citacoes\": formatar_citacoes(docs_relacionados, pergunta),\n",
        "            \"contexto_encontrado\": True}"
      ],
      "metadata": {
        "id": "q2vYhVQVuAse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "uymGttUXuB_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "    resposta = perguntar_politica_RAG(msg_teste)\n",
        "    print(f\"PERGUNTA: {msg_teste}\")\n",
        "    print(f\"RESPOSTA: {resposta['answer']}\")\n",
        "    if resposta['contexto_encontrado']:\n",
        "        print(\"CITAÇÕES:\")\n",
        "        for c in resposta['citacoes']:\n",
        "            print(f\" - Documento: {c['documento']}, Página: {c['pagina']}\")\n",
        "            print(f\"   Trecho: {c['trecho']}\")\n",
        "        print(\"------------------------------------\")"
      ],
      "metadata": {
        "id": "xCY8kGJ-uDIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 03**"
      ],
      "metadata": {
        "id": "gD-6Fj29p56n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langgraph"
      ],
      "metadata": {
        "id": "SsYe-7ppuEUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Optional\n",
        "\n",
        "class AgentState(TypedDict, total = False):\n",
        "    pergunta: str\n",
        "    triagem: dict\n",
        "    resposta: Optional[str]\n",
        "    citacoes: List[dict]\n",
        "    rag_sucesso: bool\n",
        "    acao_final: str"
      ],
      "metadata": {
        "id": "zdT9y7OoqAPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_triagem(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de triagem...\")\n",
        "    return {\"triagem\": triagem(state[\"pergunta\"])}\n",
        "\n"
      ],
      "metadata": {
        "id": "Cj__gBlD3Gki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_auto_resolver(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de auto_resolver...\")\n",
        "    resposta_rag = perguntar_politica_RAG(state[\"pergunta\"])\n",
        "\n",
        "    update: AgentState = {\n",
        "        \"resposta\": resposta_rag[\"answer\"],\n",
        "        \"citacoes\": resposta_rag.get(\"citacoes\", []),\n",
        "        \"rag_sucesso\": resposta_rag[\"contexto_encontrado\"],\n",
        "    }\n",
        "\n",
        "    if resposta_rag[\"contexto_encontrado\"]:\n",
        "        update[\"acao_final\"] = \"AUTO_RESOLVER\"\n",
        "\n",
        "    return update"
      ],
      "metadata": {
        "id": "5Gp7U_QD3ISQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_pedir_info(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de pedir_info...\")\n",
        "    faltantes = state[\"triagem\"].get(\"campos_faltantes\", [])\n",
        "    if faltantes:\n",
        "        detalhe = \",\".join(faltantes)\n",
        "    else:\n",
        "        detalhe = \"Tema e contexto específico\"\n",
        "\n",
        "    return {\n",
        "        \"resposta\": f\"Para avançar, preciso que detalhe: {detalhe}\",\n",
        "        \"citacoes\": [],\n",
        "        \"acao_final\": \"PEDIR_INFO\"\n",
        "    }"
      ],
      "metadata": {
        "id": "d9s0F7Uo3Jmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_abrir_chamado(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de abrir_chamado...\")\n",
        "    triagem = state[\"triagem\"]\n",
        "\n",
        "    return {\n",
        "        \"resposta\": f\"Abrindo chamado com urgência {triagem['urgencia']}. Descrição: {state['pergunta'][:140]}\",\n",
        "        \"citacoes\": [],\n",
        "        \"acao_final\": \"ABRIR_CHAMADO\"\n",
        "    }"
      ],
      "metadata": {
        "id": "EPe5tjOM3LxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORDS_ABRIR_TICKET = [\"aprovação\", \"exceção\", \"liberação\", \"abrir ticket\", \"abrir chamado\", \"acesso especial\"]\n",
        "\n",
        "def decidir_pos_triagem(state: AgentState) -> str:\n",
        "    print(\"Decidindo após a triagem...\")\n",
        "    decisao = state[\"triagem\"][\"decisao\"]\n",
        "\n",
        "    if decisao == \"AUTO_RESOLVER\": return \"auto\"\n",
        "    if decisao == \"PEDIR_INFO\": return \"info\"\n",
        "    if decisao == \"ABRIR_CHAMADO\": return \"chamado\""
      ],
      "metadata": {
        "id": "UZMibbon3M69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decidir_pos_auto_resolver(state: AgentState) -> str:\n",
        "    print(\"Decidindo após o auto_resolver...\")\n",
        "\n",
        "    if state.get(\"rag_sucesso\"):\n",
        "        print(\"Rag com sucesso, finalizando o fluxo.\")\n",
        "        return \"ok\"\n",
        "\n",
        "    state_da_pergunta = (state[\"pergunta\"] or \"\").lower()\n",
        "\n",
        "    if any(k in state_da_pergunta for k in KEYWORDS_ABRIR_TICKET):\n",
        "        print(\"Rag falhou, mas foram encontradas keywords de abertura de ticket. Abrindo...\")\n",
        "        return \"chamado\"\n",
        "\n",
        "    print(\"Rag falhou, sem keywords, vou pedir mais informações...\")\n",
        "    return \"info\""
      ],
      "metadata": {
        "id": "MY0Uo5kC3OTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"triagem\", node_triagem)\n",
        "workflow.add_node(\"auto_resolver\", node_auto_resolver)\n",
        "workflow.add_node(\"pedir_info\", node_pedir_info)\n",
        "workflow.add_node(\"abrir_chamado\", node_abrir_chamado)\n",
        "\n",
        "workflow.add_edge(START, \"triagem\")\n",
        "workflow.add_conditional_edges(\"triagem\", decidir_pos_triagem, {\n",
        "    \"auto\": \"auto_resolver\",\n",
        "    \"info\": \"pedir_info\",\n",
        "    \"chamado\": \"abrir_chamado\"\n",
        "})\n",
        "\n",
        "workflow.add_conditional_edges(\"auto_resolver\", decidir_pos_auto_resolver, {\n",
        "    \"info\": \"pedir_info\",\n",
        "    \"chamado\": \"abrir_chamado\",\n",
        "    \"ok\": END\n",
        "})\n",
        "\n",
        "workflow.add_edge(\"pedir_info\", END)\n",
        "workflow.add_edge(\"abrir_chamado\", END)\n",
        "\n",
        "grafo = workflow.compile()"
      ],
      "metadata": {
        "id": "sYPJ3UCs3PhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "\n",
        "graph_bytes = grafo.get_graph().draw_mermaid_png()\n",
        "display(Image(graph_bytes))"
      ],
      "metadata": {
        "id": "f1E3GGPA3Qv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"É possível reembolsar certificações do Google Cloud?\",\n",
        "          \"Posso obter o Google Gemini de graça?\",\n",
        "          \"Qual é a palavra-chave da aula de hoje?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "9PdVYRPa3R_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_test in testes:\n",
        "    resposta_final = grafo.invoke({\"pergunta\": msg_test})\n",
        "\n",
        "    triag = resposta_final.get(\"triagem\", {})\n",
        "    print(f\"PERGUNTA: {msg_test}\")\n",
        "    print(f\"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}\")\n",
        "    print(f\"RESPOSTA: {resposta_final.get('resposta')}\")\n",
        "    if resposta_final.get(\"citacoes\"):\n",
        "        print(\"CITAÇÕES:\")\n",
        "        for citacao in resposta_final.get(\"citacoes\"):\n",
        "            print(f\" - Documento: {citacao['documento']}, Página: {citacao['pagina']}\")\n",
        "            print(f\"   Trecho: {citacao['trecho']}\")\n",
        "\n",
        "    print(\"------------------------\")"
      ],
      "metadata": {
        "id": "hBOboUIT3Tit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZCHUux_3WLJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}